{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from ds_utils import *\n",
    "from torch_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "ROOT_DIR = '../../'\n",
    "experiment_name = 'training_single_split'\n",
    "learning_rate = 0.00001\n",
    "epochs = 80\n",
    "patience = 10\n",
    "resnet_size = 34 # allowed sizes: 18,34,50,101,152\n",
    "num_workers = 4\n",
    "batch_size = 64\n",
    "weighted_loss = True\n",
    "#img_aug = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "prob_aug = 0.5\n",
    "sometimes = lambda augmentation: iaa.Sometimes(prob_aug, augmentation)\n",
    "img_aug = iaa.Sequential([\n",
    "    iaa.Fliplr(prob_aug),\n",
    "    sometimes(iaa.Crop(percent=(0, 0.2))),\n",
    "    sometimes(iaa.ChangeColorTemperature((1100, 10000))),\n",
    "\n",
    "    sometimes(iaa.OneOf([\n",
    "        iaa.GaussianBlur(sigma=(0, 2.0)),\n",
    "        iaa.AddToHueAndSaturation((-10, 10))\n",
    "\n",
    "    ]))\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into a dataframe containing the path and the category of each image\n",
    "df = path2DataFrame('/home/jcejudo/training_data_3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jcejudo/training_data_3000/building/[ph]...</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jcejudo/training_data_3000/building/[ph]...</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jcejudo/training_data_3000/building/[ph]...</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jcejudo/training_data_3000/building/[ph]...</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jcejudo/training_data_3000/building/[ph]...</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  category\n",
       "0  /home/jcejudo/training_data_3000/building/[ph]...  building\n",
       "1  /home/jcejudo/training_data_3000/building/[ph]...  building\n",
       "2  /home/jcejudo/training_data_3000/building/[ph]...  building\n",
       "3  /home/jcejudo/training_data_3000/building/[ph]...  building\n",
       "4  /home/jcejudo/training_data_3000/building/[ph]...  building"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply label encoding https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "X = df['file_path'].values\n",
    "y = df['category'].values\n",
    "y_encoded, class_index_dict = label_encoding(y)\n",
    "n_classes = len(class_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'archaeological_site',\n",
       " 1: 'building',\n",
       " 2: 'ceramics',\n",
       " 3: 'clothing',\n",
       " 4: 'costume_accessories',\n",
       " 5: 'drawing',\n",
       " 6: 'furniture',\n",
       " 7: 'inscription',\n",
       " 8: 'jewellery',\n",
       " 9: 'map',\n",
       " 10: 'painting',\n",
       " 11: 'photograph',\n",
       " 12: 'postcard',\n",
       " 13: 'sculpture',\n",
       " 14: 'specimen',\n",
       " 15: 'tapestry',\n",
       " 16: 'textile',\n",
       " 17: 'toy',\n",
       " 18: 'weaponry',\n",
       " 19: 'woodwork'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size train: 37711\n",
      "size val: 4191\n",
      "size test: 4656\n"
     ]
    }
   ],
   "source": [
    "#split the data into train, validation and test set\n",
    "\n",
    "#this function makes several splits\n",
    "data_splits = make_train_val_test_splits(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    img_aug = img_aug,\n",
    "    num_workers = num_workers,\n",
    "    batch_size = batch_size,\n",
    "    splits = 10,\n",
    ")\n",
    "\n",
    "#pick a single split\n",
    "split = data_splits[0]\n",
    "\n",
    "trainloader = split['trainloader']\n",
    "valloader = split['valloader']\n",
    "testloader = split['testloader']\n",
    "\n",
    "print('size train: {}'.format(len(trainloader.dataset)))\n",
    "print('size val: {}'.format(len(valloader.dataset)))\n",
    "print('size test: {}'.format(len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "#initialize model\n",
    "model = ResNet(resnet_size,n_classes).to(device)\n",
    "#set optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#set loss\n",
    "if weighted_loss:\n",
    "    #A weighted loss can be used for addressing the imbalance between categories\n",
    "    #Categories with a small number of categories are assigned a higher loss so the model is force to correctly classify them \n",
    "    weights = get_class_weights(y_encoded,class_index_dict)\n",
    "    loss_function = nn.CrossEntropyLoss(reduction ='sum',weight=torch.FloatTensor(weights).to(device))           \n",
    "else:\n",
    "    loss_function = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "#create directories for results\n",
    "experiment_path = os.path.join(ROOT_DIR,experiment_name)\n",
    "create_dir(experiment_path)\n",
    "split_path = os.path.join(experiment_path,'split')\n",
    "create_dir(split_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 80 epochs \n",
      "\n",
      "[0] train loss: 73.927 validation loss: 69.024 acc: 0.518 f1: 0.400 precision: 0.441 recall: 0.476\n",
      "[1] train loss: 68.660 validation loss: 67.383 acc: 0.576 f1: 0.457 precision: 0.455 recall: 0.528\n",
      "[2] train loss: 67.425 validation loss: 66.614 acc: 0.627 f1: 0.509 precision: 0.501 recall: 0.571\n",
      "[3] train loss: 66.469 validation loss: 66.068 acc: 0.665 f1: 0.541 precision: 0.559 recall: 0.601\n",
      "[4] train loss: 65.662 validation loss: 65.105 acc: 0.714 f1: 0.595 precision: 0.576 recall: 0.639\n",
      "[5] train loss: 64.988 validation loss: 64.804 acc: 0.729 f1: 0.607 precision: 0.584 recall: 0.651\n",
      "[6] train loss: 64.528 validation loss: 64.439 acc: 0.759 f1: 0.651 precision: 0.647 recall: 0.675\n",
      "[7] train loss: 64.063 validation loss: 64.148 acc: 0.772 f1: 0.666 precision: 0.662 recall: 0.682\n",
      "[8] train loss: 63.709 validation loss: 63.864 acc: 0.783 f1: 0.679 precision: 0.673 recall: 0.695\n",
      "[9] train loss: 63.510 validation loss: 63.887 acc: 0.788 f1: 0.682 precision: 0.672 recall: 0.700\n",
      "[10] train loss: 63.274 validation loss: 63.582 acc: 0.797 f1: 0.689 precision: 0.678 recall: 0.707\n",
      "[11] train loss: 63.083 validation loss: 63.592 acc: 0.796 f1: 0.688 precision: 0.677 recall: 0.705\n",
      "[12] train loss: 62.908 validation loss: 63.567 acc: 0.804 f1: 0.693 precision: 0.681 recall: 0.710\n",
      "[13] train loss: 62.815 validation loss: 63.408 acc: 0.806 f1: 0.693 precision: 0.679 recall: 0.712\n",
      "[14] train loss: 62.723 validation loss: 63.292 acc: 0.812 f1: 0.702 precision: 0.693 recall: 0.716\n",
      "[15] train loss: 62.564 validation loss: 63.187 acc: 0.811 f1: 0.700 precision: 0.689 recall: 0.716\n",
      "[16] train loss: 62.469 validation loss: 63.330 acc: 0.815 f1: 0.705 precision: 0.695 recall: 0.720\n",
      "[17] train loss: 62.349 validation loss: 63.029 acc: 0.819 f1: 0.707 precision: 0.696 recall: 0.723\n",
      "[18] train loss: 62.272 validation loss: 63.295 acc: 0.815 f1: 0.705 precision: 0.693 recall: 0.720\n",
      "[19] train loss: 62.135 validation loss: 63.278 acc: 0.817 f1: 0.705 precision: 0.695 recall: 0.719\n",
      "[20] train loss: 62.115 validation loss: 63.124 acc: 0.820 f1: 0.708 precision: 0.696 recall: 0.724\n",
      "[21] train loss: 61.970 validation loss: 62.941 acc: 0.832 f1: 0.763 precision: 0.759 recall: 0.770\n",
      "[22] train loss: 61.798 validation loss: 62.882 acc: 0.837 f1: 0.769 precision: 0.764 recall: 0.779\n",
      "[23] train loss: 61.732 validation loss: 62.742 acc: 0.832 f1: 0.768 precision: 0.761 recall: 0.779\n",
      "[24] train loss: 61.693 validation loss: 62.958 acc: 0.832 f1: 0.766 precision: 0.759 recall: 0.776\n",
      "[25] train loss: 61.632 validation loss: 62.955 acc: 0.836 f1: 0.770 precision: 0.763 recall: 0.780\n",
      "[26] train loss: 61.583 validation loss: 62.808 acc: 0.840 f1: 0.775 precision: 0.804 recall: 0.788\n",
      "[27] train loss: 61.460 validation loss: 62.654 acc: 0.848 f1: 0.805 precision: 0.809 recall: 0.808\n",
      "[28] train loss: 61.356 validation loss: 62.798 acc: 0.847 f1: 0.803 precision: 0.807 recall: 0.806\n",
      "[29] train loss: 61.297 validation loss: 62.598 acc: 0.850 f1: 0.807 precision: 0.810 recall: 0.809\n",
      "[30] train loss: 61.222 validation loss: 62.739 acc: 0.854 f1: 0.816 precision: 0.820 recall: 0.815\n",
      "[31] train loss: 61.221 validation loss: 62.631 acc: 0.852 f1: 0.812 precision: 0.814 recall: 0.814\n",
      "[32] train loss: 61.167 validation loss: 62.883 acc: 0.856 f1: 0.814 precision: 0.817 recall: 0.814\n",
      "[33] train loss: 61.147 validation loss: 62.638 acc: 0.852 f1: 0.810 precision: 0.812 recall: 0.813\n",
      "[34] train loss: 61.051 validation loss: 62.631 acc: 0.858 f1: 0.821 precision: 0.869 recall: 0.822\n",
      "[35] train loss: 61.030 validation loss: 62.392 acc: 0.867 f1: 0.857 precision: 0.871 recall: 0.851\n",
      "[36] train loss: 60.946 validation loss: 62.573 acc: 0.862 f1: 0.856 precision: 0.869 recall: 0.849\n",
      "[37] train loss: 60.805 validation loss: 62.313 acc: 0.872 f1: 0.869 precision: 0.879 recall: 0.863\n",
      "[38] train loss: 60.812 validation loss: 62.331 acc: 0.872 f1: 0.873 precision: 0.880 recall: 0.867\n",
      "[39] train loss: 60.751 validation loss: 62.206 acc: 0.873 f1: 0.872 precision: 0.880 recall: 0.867\n",
      "[40] train loss: 60.742 validation loss: 62.581 acc: 0.870 f1: 0.867 precision: 0.872 recall: 0.863\n",
      "[41] train loss: 60.711 validation loss: 62.288 acc: 0.867 f1: 0.865 precision: 0.870 recall: 0.862\n",
      "[42] train loss: 60.662 validation loss: 62.253 acc: 0.873 f1: 0.870 precision: 0.875 recall: 0.867\n",
      "[43] train loss: 60.604 validation loss: 62.057 acc: 0.882 f1: 0.879 precision: 0.884 recall: 0.875\n",
      "[44] train loss: 60.602 validation loss: 62.348 acc: 0.872 f1: 0.873 precision: 0.878 recall: 0.869\n",
      "[45] train loss: 60.634 validation loss: 62.280 acc: 0.875 f1: 0.877 precision: 0.887 recall: 0.870\n",
      "[46] train loss: 60.537 validation loss: 62.290 acc: 0.876 f1: 0.875 precision: 0.878 recall: 0.873\n",
      "[47] train loss: 60.483 validation loss: 62.351 acc: 0.868 f1: 0.865 precision: 0.867 recall: 0.866\n",
      "[48] train loss: 60.494 validation loss: 62.268 acc: 0.877 f1: 0.874 precision: 0.876 recall: 0.873\n"
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    model = model,\n",
    "    loss_function = loss_function,\n",
    "    optimizer = optimizer,\n",
    "    trainloader = trainloader,\n",
    "    valloader = valloader,\n",
    "    device = device,\n",
    "    saving_dir = split_path,\n",
    "    epochs = epochs,\n",
    "    patience = patience)\n",
    "\n",
    "\n",
    "#save model data and training history\n",
    "experiment = Experiment()\n",
    "experiment.add('class_index_dict',class_index_dict)\n",
    "experiment.add('model',model)\n",
    "experiment.add('resnet_size',resnet_size)\n",
    "\n",
    "for k,v in history.items():\n",
    "    experiment.add(k,v)\n",
    "\n",
    "experiment.save(split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_list = ['accuracy','f1','precision','recall','sensitivity','specificity']\n",
    "max_epochs = 5\n",
    "training_history_dict = read_train_history(experiment_path,metrics_list,max_epochs)\n",
    "\n",
    "#plot loss on train and validation\n",
    "plot_mean_std({metric:np.array(training_history_dict[metric]) for metric in ['loss_val','loss_train']},'',fontsize=20)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot metrics on validation data\n",
    "plot_mean_std({metric:np.array(training_history_dict[metric]) for metric in metrics_list},'',fontsize=20,y_lim = 1.0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "test_metrics, ground_truth_list, predictions_list,test_images_list = evaluate(model = model,\n",
    "                                                                              dataloader = testloader,\n",
    "                                                                              device = device,\n",
    "                                                                              loss_function = loss_function)\n",
    "for k in metrics_list:\n",
    "    print(f'{k}_test: {test_metrics[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize confusion matrix\n",
    "confusion_matrix = test_metrics['confusion_matrix']\n",
    "labels = [class_index_dict[i] for i in range(confusion_matrix.shape[0])]\n",
    "plot_conf_matrix(confusion_matrix,labels,font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
