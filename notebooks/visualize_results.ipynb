{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from torch_utils import *\n",
    "from pathlib import Path\n",
    "from multilabel.evaluate import *\n",
    "from harvest_data import EuropeanaAPI\n",
    "\n",
    "from train_multilabel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_path = '/home/jcejudo/projects/image_classification/results/single_label/crossvalidation'\n",
    "\n",
    "results_path = Path(results_path)\n",
    "metrics_list = ['accuracy','precision','recall','f1',]\n",
    "\n",
    "metrics_dict = {k:[] for k in metrics_list}\n",
    "for split in results_path.iterdir():\n",
    "    info_dict_path = split.joinpath('training_info.pth')\n",
    "    info_dict = torch.load(info_dict_path)\n",
    "    \n",
    "    class_index_path = split.joinpath('class_index.json')\n",
    "    \n",
    "    with open(class_index_path,'r') as f:\n",
    "        class_index_dict = json.load(f)\n",
    "        class_index_dict = {int(i):v for i,v in class_index_dict.items()}\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        metrics_dict[metric].append(info_dict[metric+'_test'])\n",
    "               \n",
    "for k,v in metrics_dict.items():\n",
    "    mean = sum(v)/len(v)\n",
    "    std = np.std(np.array(v))\n",
    "    print(f'{k}: {mean:.3f}+-{std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix = info_dict['confusion_matrix_test']\n",
    "labels = [class_index_dict[i] for i in range(confusion_matrix.shape[0])]\n",
    "plot_conf_matrix(confusion_matrix,labels,font_scale=2.0,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_results = '/home/jcejudo/projects/image_classification/results/single_label/evaluation'\n",
    "\n",
    "results_path = Path(eval_results).joinpath('evaluation_results.pth')\n",
    "metrics_dict = torch.load(results_path)\n",
    "for k,v in metrics_dict.items():\n",
    "    if k != 'confusion_matrix':\n",
    "        print(f'{k}: {v:.3f}')\n",
    "        \n",
    "labels = [class_index_dict[i] for i in range(confusion_matrix.shape[0])]\n",
    "plot_conf_matrix(metrics_dict['confusion_matrix'],labels,font_scale=2.0,figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to do: gradcam correctly classified, misclassified\n",
    "\n",
    "results_path = '/home/jcejudo/projects/image_classification/results/single_label/crossvalidation'\n",
    "data_dir = '/home/jcejudo/projects/image_classification/data/single_label/images_evaluation'\n",
    "\n",
    "mode = 'incorrect'\n",
    "\n",
    "get_first = True\n",
    "\n",
    "split_path = Path(results_path).joinpath('split_0')\n",
    "with open(split_path.joinpath('conf.json'),'r') as f:\n",
    "    conf = json.load(f)\n",
    "\n",
    "input_size = conf['input_size']\n",
    "resnet_size = conf['resnet_size']\n",
    "\n",
    "df = path2DataFrame(data_dir)\n",
    "X = df['file_path'].values\n",
    "y = df['category'].values\n",
    "\n",
    "#load class_index dict\n",
    "with open(split_path.joinpath('class_index.json'),'r') as f:\n",
    "    class_index_dict = json.load(f)\n",
    "    class_index_dict = {int(i):v for i,v in class_index_dict.items()}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet(resnet_size,len(class_index_dict)).to(device)\n",
    "\n",
    "#load model\n",
    "model_path = split_path.joinpath('checkpoint.pth')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((input_size,input_size)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "cat_list = []\n",
    "\n",
    "for img_path, label in zip(X,y):\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "    ID = Path(img_path).with_suffix('').name.replace('[ph]','/')\n",
    "    URI = 'http://data.europeana.eu/item'+ID\n",
    "    \n",
    "    category_list, confidence_list, XAI_list = predict_grad_cam(\n",
    "        model = model, \n",
    "        class_index_dict = class_index_dict,\n",
    "        image = image,\n",
    "        heatmap_layer = model.net.layer4[1].conv2, \n",
    "        transform = transform, \n",
    "        device = device, \n",
    "        thres = 0.1, \n",
    "        max_pred = 3)\n",
    "\n",
    "    pred = category_list[0]\n",
    "    conf = confidence_list[0]\n",
    "\n",
    "    if pred == label and mode == 'correct' or pred != label and mode == 'incorrect':\n",
    "        \n",
    "        if get_first:\n",
    "            if label in cat_list:\n",
    "                continue\n",
    "            else:\n",
    "                cat_list.append(label)\n",
    "            \n",
    "        \n",
    "        \n",
    "        print(URI)\n",
    "        print('ground truth: ',label)\n",
    "        plot_grad_cam(\n",
    "            image = image,\n",
    "            category_list = category_list, \n",
    "            confidence_list = confidence_list,\n",
    "            XAI_list = XAI_list,\n",
    "            fontsize = 20,\n",
    "            figsize = (20,20)\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "        print(50*'--')\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_path = '/home/jcejudo/projects/image_classification/results/multilabel/crossvalidation'\n",
    "results_path = Path(results_path)\n",
    "\n",
    "metrics_list = ['coverage','lrap','label_ranking_loss','ndcg_score','dcg_score']\n",
    "metrics_list += ['acc','precision','recall','f1',]\n",
    "\n",
    "metrics_dict = {k:[] for k in metrics_list}\n",
    "for split in results_path.iterdir():\n",
    "    info_dict_path = split.joinpath('training_info.pth')\n",
    "    info_dict = torch.load(info_dict_path)\n",
    "    \n",
    "    class_index_path = split.joinpath('class_index.json')\n",
    "    \n",
    "    with open(class_index_path,'r') as f:\n",
    "        class_index_dict = json.load(f)\n",
    "        class_index_dict = {int(i):v for i,v in class_index_dict.items()}\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        metrics_dict[metric].append(info_dict[metric+'_test'])\n",
    "               \n",
    "for k,v in metrics_dict.items():\n",
    "    mean = sum(v)/len(v)\n",
    "    std = np.std(np.array(v))\n",
    "    print(f'{k}: {mean:.3f}+-{std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_metrics = {k:{'precision':[],'recall':[],'f1-score':[],} for k in class_index_dict.values()}\n",
    "\n",
    "for split in results_path.iterdir():\n",
    "    info_dict_path = split.joinpath('training_info.pth')\n",
    "    info_dict = torch.load(info_dict_path)\n",
    "    \n",
    "    class_index_path = split.joinpath('class_index.json')\n",
    "\n",
    "    for cm,v in zip(info_dict['confusion_matrix_test'],class_index_dict.values()):\n",
    "\n",
    "        TN = float(cm[0,0])\n",
    "        TP = float(cm[1,1])\n",
    "        FP = float(cm[0,1])\n",
    "        FN = float(cm[1,0])\n",
    "\n",
    "        try:\n",
    "            recall = TP/(TP+FN)\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            precision = TP/(TP+FP)\n",
    "        except:\n",
    "            precision = 0\n",
    "            \n",
    "        try:\n",
    "            f1 = 2*TP/(2*TP+FP+FN)\n",
    "        except:\n",
    "            f1 = 0\n",
    "            \n",
    "        cat_metrics[v]['precision'].append(precision)\n",
    "        cat_metrics[v]['recall'].append(recall)\n",
    "        cat_metrics[v]['f1-score'].append(f1)\n",
    "        \n",
    "\n",
    "for k in cat_metrics.keys():\n",
    "    cat_metrics[k]['precision'] = {'mean':np.mean(cat_metrics[k]['precision']),'std':np.std(cat_metrics[k]['precision'])}\n",
    "    cat_metrics[k]['recall'] = {'mean':np.mean(cat_metrics[k]['recall']),'std':np.std(cat_metrics[k]['recall'])}\n",
    "    cat_metrics[k]['f1-score'] = {'mean':np.mean(cat_metrics[k]['f1-score']),'std':np.std(cat_metrics[k]['f1-score'])}\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "table_list = [['category','precision','recall','f1-score']]\n",
    "\n",
    "\n",
    "for k in cat_metrics.keys():\n",
    "    prec_mean = cat_metrics[k]['precision']['mean']\n",
    "    prec_std = cat_metrics[k]['precision']['std']\n",
    "    prec_str = f'{prec_mean:.3f}+-{prec_std:.3f}'\n",
    "    \n",
    "    recall_mean = cat_metrics[k]['recall']['mean']\n",
    "    recall_std = cat_metrics[k]['recall']['std']\n",
    "    recall_str = f'{recall_mean:.3f}+-{recall_std:.3f}'\n",
    "    \n",
    "    f1_mean = cat_metrics[k]['f1-score']['mean']\n",
    "    f1_std = cat_metrics[k]['f1-score']['std']\n",
    "    f1_str = f'{f1_mean:.3f}+-{f1_std:.3f}'\n",
    "    \n",
    "    table_list.append([k,prec_str,recall_str,f1_str])\n",
    "\n",
    "print(tabulate(table_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "for cm,v in zip(info_dict['confusion_matrix_test'],class_index_dict.values()):\n",
    "    labels = ['False','True']\n",
    "    plot_conf_matrix(cm,labels,font_scale=2.0,figsize=(5,5),title=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = '/home/jcejudo/projects/image_classification/results/multilabel/evaluation'\n",
    "\n",
    "results_path = Path(eval_results).joinpath('evaluation_results.pth')\n",
    "metrics_dict = torch.load(results_path)\n",
    "for k,v in metrics_dict.items():\n",
    "    if k not in ['confusion_matrix']:\n",
    "        try:\n",
    "            print(f'{str(k)}: {v:.3f}')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "for cm,v in zip(metrics_dict['confusion_matrix'],class_index_dict.values()):\n",
    "    labels = ['False','True']\n",
    "    plot_conf_matrix(cm,labels,font_scale=2.0,figsize=(5,5),title=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradcam \n",
    "\n",
    "results_path = '/home/jcejudo/projects/image_classification/results/multilabel/crossvalidation'\n",
    "data_dir = '/home/jcejudo/projects/image_classification/data/multilabel/images_evaluation'\n",
    "eval_annotations = '/home/jcejudo/projects/image_classification/data/multilabel/eval_multilabel_with_URL.csv'\n",
    "\n",
    "data_dir = Path(data_dir)\n",
    "\n",
    "split_path = Path(results_path).joinpath('split_0')\n",
    "with open(split_path.joinpath('conf.json'),'r') as f:\n",
    "    conf = json.load(f)\n",
    "\n",
    "input_size = conf['input_size']\n",
    "resnet_size = conf['resnet_size']\n",
    "\n",
    "#load class_index dict\n",
    "with open(split_path.joinpath('class_index.json'),'r') as f:\n",
    "    class_index_dict = json.load(f)\n",
    "    class_index_dict = {int(i):v for i,v in class_index_dict.items()}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model,class_index_dict = load_multilabel_model(split_path,device,resnet_size = resnet_size)\n",
    "\n",
    "df = pd.read_csv(eval_annotations)\n",
    "#df = df.dropna()\n",
    "#filter images in df contained in data_path\n",
    "imgs_list = list(data_dir.iterdir())\n",
    "df['filepath'] = df['ID'].apply(lambda x:data_dir.joinpath(id_to_filename(x)+'.jpg'))\n",
    "df = df.loc[df['filepath'].apply(lambda x: Path(x) in imgs_list)]\n",
    "\n",
    "X = df['filepath'].values\n",
    "y = df['category'].values\n",
    "\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((input_size,input_size)),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "for img_path, label in zip(X,y):\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "    ID = Path(img_path).with_suffix('').name.replace('[ph]','/')\n",
    "    URI = 'http://data.europeana.eu/item'+ID\n",
    "    \n",
    "    category_list, confidence_list, XAI_list = predict_grad_cam(\n",
    "        model = model, \n",
    "        class_index_dict = class_index_dict,\n",
    "        image = image,\n",
    "        heatmap_layer = model.net.layer4[1].conv2, \n",
    "        transform = transform, \n",
    "        device = device, \n",
    "        thres = 0.5, \n",
    "        max_pred = 3)\n",
    "\n",
    "    print(URI)\n",
    "    print('ground truth: ',label)\n",
    "    plot_grad_cam(\n",
    "        image = image,\n",
    "        category_list = category_list, \n",
    "        confidence_list = confidence_list,\n",
    "        XAI_list = XAI_list,\n",
    "        fontsize = 20,\n",
    "        figsize = (20,20)\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(50*'--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read crossvalidation results\n",
    "\n",
    "results_path = '/home/jcejudo/projects/image_classification/results/multilabel/crossvalidation'\n",
    "\n",
    "results_path = Path(results_path)\n",
    "\n",
    "metrics_list = ['loss','coverage','lrap','label_ranking_loss','ndcg_score','dcg_score']\n",
    "\n",
    "metrics_dict = {k:[] for k in metrics_list}\n",
    "for split in results_path.iterdir():\n",
    "    with open(split.joinpath('test_metrics.json'),'r') as f:\n",
    "        test_metrics = json.load(f)\n",
    "    for metric in metrics_dict:\n",
    "        if metric in test_metrics.keys():\n",
    "            metrics_dict[metric].append(test_metrics[metric])\n",
    "\n",
    "        \n",
    "for k,v in metrics_dict.items():\n",
    "    mean = sum(v)/len(v)\n",
    "    std = np.std(np.array(v))\n",
    "    print(f'{k}: {mean:.3f}+-{std:.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up evaluation set\n",
    "\n",
    "def validate_categories(x):\n",
    "    return ' '.join([cat for cat in x.split() if cat in vocab_dict.keys()])\n",
    "\n",
    "def map_vocab(x,vocab_dict):\n",
    "    return ' '.join([vocab_dict[cat] for cat in x.split()])\n",
    "\n",
    "def get_url(ID):\n",
    "    return eu.record(ID)\n",
    "    \n",
    "eu = EuropeanaAPI('api2demo')\n",
    "with open('../vocabularies/vocabulary.json','r') as f:\n",
    "    vocab_dict = json.load(f)\n",
    "    \n",
    "df = pd.read_csv('../data/multilabel/eval_multilabel.csv')\n",
    "print(df.shape)\n",
    "df['category'] = df['category'].apply(lambda x: validate_categories(x))\n",
    "df['skos_concept'] = df['category'].apply(lambda x: map_vocab(x,vocab_dict))\n",
    "df['URL'] = df['ID'].apply(lambda x: get_url(x))\n",
    "print(df.shape)\n",
    "df.to_csv('../data/multilabel/eval_multilabel_with_URL.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/multilabel/training_data.csv')\n",
    "df.columns\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['category'] == ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(**kwargs):\n",
    "\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "    \n",
    "    model = kwargs.get('model')\n",
    "    dataloader = kwargs.get('dataloader')\n",
    "    loss_function = kwargs.get('loss_function')\n",
    "    device = kwargs.get('device')\n",
    "    \n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for inputs,labels,_ in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        output = model(inputs)\n",
    "        labels = labels.type_as(output)\n",
    "        loss = loss_function(output, labels)\n",
    "        val_loss += loss.item()\n",
    "                \n",
    "        ground_truth += list(labels.cpu().detach().numpy())\n",
    "        predictions += list(output.cpu().detach().numpy())\n",
    "        \n",
    "    ground_truth = np.array(ground_truth)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    print(ground_truth.shape)\n",
    "        \n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    coverage = sklearn.metrics.coverage_error(ground_truth, predictions)\n",
    "    lrap = sklearn.metrics.label_ranking_average_precision_score(ground_truth, predictions)\n",
    "    label_ranking_loss = sklearn.metrics.label_ranking_loss(ground_truth, predictions)\n",
    "    ndcg_score = sklearn.metrics.ndcg_score(ground_truth, predictions)\n",
    "    dcg_score = sklearn.metrics.dcg_score(ground_truth, predictions)\n",
    "\n",
    "    #sklearn.metrics.multilabel_confusion_matrix(ground_truth, predictions,labels=np.arange(output.shape[1]))\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'loss':val_loss,\n",
    "        'coverage':coverage,\n",
    "        'lrap':lrap,\n",
    "        'label_ranking_loss':label_ranking_loss,\n",
    "        'ndcg_score':ndcg_score,\n",
    "        'dcg_score':dcg_score,\n",
    "        },ground_truth,predictions\n",
    "\n",
    "\n",
    "#analysis evaluation set\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "input_size = 128\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "model_dir = '/home/jcejudo/results_multilabel_no_test'\n",
    "data_dir = '/home/jcejudo/eval_multilabel'\n",
    "annotations = '../data/multilabel/eval_multilabel.csv'\n",
    "\n",
    "\n",
    "data_dir = Path(data_dir)\n",
    "df_path = Path(annotations)\n",
    "model_dir = Path(model_dir)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.Resize((input_size, input_size)),\n",
    "  transforms.ToTensor(),\n",
    "  # this normalization is required https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model,class_index_dict = load_multilabel_model(model_dir,device)\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "print(df)\n",
    "#filter images in df contained in data_path\n",
    "imgs_list = list(data_dir.iterdir())\n",
    "df['filepath'] = df['ID'].apply(lambda x:data_dir.joinpath(id_to_filename(x)+'.jpg'))\n",
    "df = df.loc[df['filepath'].apply(lambda x: Path(x) in imgs_list)]\n",
    "\n",
    "\n",
    "mlb = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "mlb.fit([class_index_dict.values()])\n",
    "\n",
    "imgs = np.array([str(path) for path in df['filepath'].values])\n",
    "labels = [item.split() for item in df['category'].values]\n",
    "labels = mlb.transform(labels)\n",
    "\n",
    "testset = MultilabelDataset(imgs,labels,transform = test_transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,shuffle=True, num_workers=num_workers,drop_last=True)\n",
    "\n",
    "print('test:',imgs.shape[0])\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "test_metrics,ground_truth,predictions = evaluate(\n",
    "  model = model,\n",
    "  dataloader = testloader,\n",
    "  loss_function = loss_function,\n",
    "  device = device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "\n",
    "true_positive_dict = {}\n",
    "false_negative_dict = {}\n",
    "false_positive_dict = {}\n",
    "\n",
    "for gt,pred in zip(ground_truth,predictions):\n",
    "    gt = [class_index_dict[i] for i in np.where(gt  == 1.0)[0]]\n",
    "    pred = [class_index_dict[i] for i in np.where(pred  > threshold)[0]]\n",
    "    \n",
    "    for label in gt:\n",
    "        if label not in pred:\n",
    "            if label not in false_negative_dict:\n",
    "                false_negative_dict.update({label:1})\n",
    "            else:\n",
    "                false_negative_dict[label] += 1\n",
    "                \n",
    "        else:\n",
    "            if label not in true_positive_dict:\n",
    "                true_positive_dict.update({label:1})\n",
    "            else:\n",
    "                true_positive_dict[label] += 1\n",
    "            \n",
    "    for label in pred:\n",
    "        if label not in gt:\n",
    "            if label not in false_positive_dict:\n",
    "                false_positive_dict.update({label:1})\n",
    "            else:\n",
    "                false_positive_dict[label] += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "print('correctly classified:',true_positive_dict)\n",
    "print('false negatives:',false_negative_dict)\n",
    "print('false positives:',false_positive_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_count_dict(count_dict,title=''):\n",
    "    cats = []\n",
    "    values = []\n",
    "    for k,v in count_dict.items():\n",
    "        cats.append(k)\n",
    "        values.append(v)\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "    pos = np.arange(len(cats))\n",
    "    rects = ax.barh(pos, values,\n",
    "                     align='center',\n",
    "                     height=0.5,\n",
    "                     tick_label=cats)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        ax.text(v + 0.5, i, str(v))\n",
    "        \n",
    "    ax.set_title(title)\n",
    "        \n",
    "plot_count_dict(true_positive_dict,'true positives')\n",
    "plot_count_dict(false_negative_dict,'false negatives')\n",
    "plot_count_dict(false_positive_dict,'false positives')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
